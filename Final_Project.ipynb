{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CRAUGUTH/nnProject/blob/main/Final_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e55DaojVMuVI"
      },
      "source": [
        "# **Training Neural Network**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In terms of running our model there should be no probelms when simply just running the cells within the 'Training Neural Network' folder. This folder will define the model, train/evaluate the model, and then display the results of the model. In terms of then running the Spotify intergration there is a bit of a loop you have to jump through but the instructions for that are above the code cell once you get to it."
      ],
      "metadata": {
        "id": "OD1irGDrzIUU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1zUSW9LiGMc"
      },
      "source": [
        "## Installing and Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4PYu6ivkHzjj"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade spotipy torch pandas scikit-learn transformers librosa joblib\n",
        "\n",
        "import spotipy\n",
        "from spotipy.oauth2 import SpotifyClientCredentials\n",
        "import torch\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "import joblib\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrcpPGt7lNnG"
      },
      "source": [
        "## Defining the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-A5yya9BukTg"
      },
      "source": [
        "Here is where the two different models are defined, the first being the default model with all the classifible attributes and the second being the model with the least amount of classifible attributes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "dn3wNUImH1RX"
      },
      "outputs": [],
      "source": [
        "class MusicDataDefault(Dataset):\n",
        "    def __init__(self, path):\n",
        "        data = pd.read_csv(path)\n",
        "        data = data.drop(['track_id'], axis=1)\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        data['track_genre'] = self.label_encoder.fit_transform(data['track_genre'])\n",
        "        self.features = data.drop(['track_genre'], axis=1).values.astype(np.float32)\n",
        "        self.labels = data['track_genre'].values.astype(np.float32)\n",
        "        self.scaler = StandardScaler()\n",
        "        self.features = self.scaler.fit_transform(self.features)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.features[idx]\n",
        "        y = self.labels[idx]\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmcxUDpmdgG3"
      },
      "source": [
        "The way that this model was determined was by taking the default model and removing one attribute at a time to see which attributes where the most important. During these test we had trained over 100 epochs per model in order to determine the accuracy of the models. Once the order of most important attribues was determined we added attributes back unitl the accuracy was about the same as than the origainal model. This allowed us to obtain a model with the highest accuracy with the fewest number of classifible attributes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "6FRmj480y8yw"
      },
      "outputs": [],
      "source": [
        "class MDN(Dataset):\n",
        "    def __init__(self, path):\n",
        "        data = pd.read_csv(path)\n",
        "        data = data.drop(['track_id', 'time_signature', 'liveness', 'key', 'mode',\n",
        "                          'speechiness', 'energy', 'duration_ms', 'instrumentalness'], axis=1)\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        data['track_genre'] = self.label_encoder.fit_transform(data['track_genre'])\n",
        "        self.features = data.drop(['track_genre'], axis=1).values.astype(np.float32)\n",
        "        self.labels = data['track_genre'].values.astype(np.float32)\n",
        "        self.scaler = StandardScaler()\n",
        "        self.features = self.scaler.fit_transform(self.features)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.features[idx]\n",
        "        y = self.labels[idx]\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GiWw8T-lZaS"
      },
      "source": [
        "## Define Underlying MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "VAtGym4giwan"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super(MLP, self).__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(input_dim, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvPvz_LfDMqB"
      },
      "source": [
        "### Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2mvleWfH94k"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "data_path = '/content/drive/My Drive/Junior/Semester_2/NN/Project/dataset.csv'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJ_ql_K7DTFH"
      },
      "source": [
        "### Define Evaluate Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "HA7BG3h0iApZ"
      },
      "outputs": [],
      "source": [
        "# Function to evaluate model performance\n",
        "def evaluate(model, data_loader):\n",
        "    model.eval()\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "    all_predicted = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data_loader:\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_samples += labels.size(0)\n",
        "            total_correct += (predicted == labels.long()).sum().item()\n",
        "            all_predicted.extend(predicted.view(-1).tolist())\n",
        "            all_labels.extend(labels.long().view(-1).tolist())\n",
        "\n",
        "    accuracy = (total_correct / total_samples) * 100\n",
        "    f1 = f1_score(all_labels, all_predicted, average='weighted')\n",
        "    return accuracy, f1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiIrxE9xDWwR"
      },
      "source": [
        "## Neural Network Training Sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZecpXpKjAFX",
        "outputId": "fde6f636-717c-4102-e843-d99f5544d220"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset MusicDataDefault, Training Accuracy: 75.90%, Validation Accuracy: 72.86%, Test Accuracy: 74.50%\n",
            "Dataset MDN, Training Accuracy: 69.21%, Validation Accuracy: 69.93%, Test Accuracy: 67.07%\n"
          ]
        }
      ],
      "source": [
        "# Load datasets\n",
        "datasets = [MusicDataDefault(data_path), MDN(data_path)]\n",
        "\n",
        "\n",
        "for dataset in datasets:\n",
        "\n",
        "    # Define best model and model accuracy\n",
        "    best_val_accuracy = 0\n",
        "    best_model_state = None\n",
        "\n",
        "    # Splitting the dataset\n",
        "    total_size = len(dataset)\n",
        "    train_set_size = int(total_size * 0.6)\n",
        "    val_set_size = int(total_size * 0.2)\n",
        "    test_set_size = total_size - (train_set_size + val_set_size)\n",
        "    train_set, val_set, test_set = random_split(dataset, [train_set_size, val_set_size, test_set_size])\n",
        "\n",
        "    train_loader = DataLoader(dataset=train_set, batch_size=64, shuffle=True)\n",
        "    val_loader = DataLoader(dataset=val_set, batch_size=64, shuffle=False)\n",
        "    test_loader = DataLoader(dataset=test_set, batch_size=64, shuffle=False)\n",
        "\n",
        "    # Model, loss, and optimizer\n",
        "    input_dim = dataset.features.shape[1]\n",
        "    num_classes = len(np.unique(dataset.labels))\n",
        "\n",
        "    model = MLP(input_dim, num_classes)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', factor=0.3, patience=5)\n",
        "\n",
        "    # Training loop\n",
        "    num_epochs = 50\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        total_correct = 0\n",
        "        total_samples = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels.long())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_correct += (predicted == labels.long()).sum().item()\n",
        "            total_samples += labels.size(0)\n",
        "\n",
        "        train_accuracy = (total_correct / total_samples) * 100\n",
        "        val_accuracy, val_f1 = evaluate(model, val_loader)\n",
        "\n",
        "        # Adjust the learning rate based on validation accuracy\n",
        "        scheduler.step(val_accuracy)\n",
        "\n",
        "        # Keeps the most accurate model\n",
        "        if val_accuracy > best_val_accuracy:\n",
        "            best_val_accuracy = val_accuracy\n",
        "            best_model_state = model.state_dict()\n",
        "\n",
        "        # Print training and validation results\n",
        "        # print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/total_samples:.4f}, Training Accuracy: {train_accuracy:.2f}%, Validation Accuracy: {val_accuracy:.2f}%, Validation F1: {val_f1:.4f}')\n",
        "\n",
        "    # After training is complete, restore the best model state\n",
        "    model.load_state_dict(best_model_state)\n",
        "\n",
        "    # Evaluate on the test set without loading any saved state\n",
        "    test_accuracy, test_f1 = evaluate(model, test_loader)\n",
        "    print(f'Dataset {dataset.__class__.__name__}, Training Accuracy: {train_accuracy:.2f}%, Validation Accuracy: {val_accuracy:.2f}%, Test Accuracy: {test_accuracy:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNBH324CiPxQ"
      },
      "source": [
        "### Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HMZKelGlHSYO"
      },
      "outputs": [],
      "source": [
        "# Save the model and preprocessors\n",
        "torch.save(model.state_dict(), 'song_genre_classifier.pth')\n",
        "joblib.dump(dataset.label_encoder, 'label_encoder.joblib')\n",
        "joblib.dump(dataset.scaler, 'scaler.joblib')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_XpAhfnDs66"
      },
      "source": [
        "# **Music Classification**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfunDko6DzFz"
      },
      "source": [
        "## Obtain Personal Spotify Playlist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Md4ubr5ID7c6"
      },
      "source": [
        "Get new access token for Spotify account. In order to use this new token you must run the cell, click the link, and then copy the text after 'code='"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4GHuj0RD9wM",
        "outputId": "d26f937d-9d63-4b1b-dad4-6f27876cf6b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://accounts.spotify.com/authorize?client_id=cce8687d5317494cb81ff7731ccd9a2b&response_type=code&redirect_uri=https%3A%2F%2Fexample.com%2Fcallback%2F&scope=playlist-modify-private\n"
          ]
        }
      ],
      "source": [
        "client = spotipy.oauth2.SpotifyOAuth(\n",
        "    client_id='cce8687d5317494cb81ff7731ccd9a2b',\n",
        "    client_secret='9646e0745dde4bddb7c67b29431a61f0',\n",
        "    redirect_uri='https://example.com/callback/',\n",
        "    scope='playlist-modify-private'\n",
        ")\n",
        "\n",
        "sp = spotipy.Spotify(auth_manager=client)\n",
        "authorization_url = client.get_authorize_url()\n",
        "print(authorization_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0DX0OQPFs-i"
      },
      "source": [
        "This is where you authorize your account, the code is where you paste the text from the link above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MNXWRiHFjuI"
      },
      "outputs": [],
      "source": [
        "code = \"AQAtKdc5s-Jbck0uvZ94x6dNf5WnD79BBj1HSfjhtOyZU0JUjk0o5V8OTn3dUmFqpqxB-BuFAy5zUH0F8hJyQO4toDhjswZfrwLNFeaqTq2Lu41gxgylU3_8H3CAA_YVIK-aNDppDYoPdOm68Xh3X7bEMHdz--Uuf64bxEE63d6jJcb_HJIEvO6gWzwa4tAg2CYT356LtiPhOA\"\n",
        "client.get_access_token(code)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kqHPdEgF0nc"
      },
      "source": [
        "Getting playlist from my account we want to classify which is the USA Top Songs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "T3SdaU6IGA3e"
      },
      "outputs": [],
      "source": [
        "# Function to get all tracks from a playlist\n",
        "def get_playlist_tracks(playlist_id):\n",
        "    results = sp.playlist_tracks(playlist_id)\n",
        "    tracks = [item['track']['id'] for item in results['items'] if item['track']]\n",
        "    while results['next']:\n",
        "        results = sp.next(results)\n",
        "        tracks.extend([item['track']['id'] for item in results['items'] if item['track']])\n",
        "    return tracks\n",
        "\n",
        "# Get all tracks from a specific playlist\n",
        "playlist_id = '6G1GkjxHj80kLSZ4ObVxiM'\n",
        "song_ids = get_playlist_tracks(playlist_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIv3cCkiGyo6"
      },
      "source": [
        "Here we are creating a dictionary of the songs within that playlist. The dictionary contains the key, which is the song_id, and then the song name, artist, and genre."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "fzOQ6eyCG3m0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a181eae-0ab7-4ffa-96e2-f62d549cac69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "don't talk 2 me! by 1upboy, \n",
            "Predicted Genre: r-n-b\n",
            "\n",
            "The End (feat. BABYMETAL) by Lil Uzi Vert, \n",
            "Predicted Genre: r-n-b\n",
            "\n",
            "Blight by Zeruel, \n",
            "Predicted Genre: r-n-b\n",
            "\n",
            "Indestructible by Disturbed, \n",
            "Predicted Genre: r-n-b\n",
            "\n",
            "War Eternal by Arch Enemy, \n",
            "Predicted Genre: r-n-b\n",
            "\n",
            "Shadows Inside by Miss May I, \n",
            "Predicted Genre: r-n-b\n",
            "\n",
            "Is This Love by Bob Marley & The Wailers, \n",
            "Predicted Genre: r-n-b\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Load the model and preprocessors\n",
        "model = MLP(input_dim, num_classes)\n",
        "model.load_state_dict(torch.load('song_genre_classifier.pth'))\n",
        "model.eval()\n",
        "\n",
        "scaler = joblib.load('scaler.joblib')\n",
        "label_encoder = joblib.load('label_encoder.joblib')\n",
        "\n",
        "results = {}\n",
        "\n",
        "for song_id in song_ids:\n",
        "    song_features = sp.audio_features(song_id)[0]\n",
        "    track_info = sp.track(song_id)\n",
        "    song_name = track_info['name']\n",
        "    artist_name = track_info['artists'][0]['name']\n",
        "    popularity = track_info['popularity']\n",
        "\n",
        "    # Ensure these features match the training set's features\n",
        "    features_list = ['danceability', 'loudness', 'acousticness', 'valence', 'tempo']\n",
        "    features_values = [song_features[feature] for feature in features_list]\n",
        "    features_values.append(popularity)\n",
        "    features_array = np.array([features_values], dtype=np.float32)\n",
        "    scaled_features = scaler.transform(features_array)\n",
        "\n",
        "    input_tensor = torch.tensor(scaled_features, dtype=torch.float32)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_tensor)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        genre = label_encoder.inverse_transform(predicted.numpy())[0]\n",
        "\n",
        "    results[song_id] = {\n",
        "        'song_name': song_name,\n",
        "        'artist_name': artist_name,\n",
        "        'genre': genre\n",
        "    }\n",
        "\n",
        "for song_id, info in results.items():\n",
        "    print(f\"{info['song_name']} by {info['artist_name']}, \\nPredicted Genre: {info['genre']}\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}