{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CRAUGUTH/nnProject/blob/main/Final_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e55DaojVMuVI"
      },
      "source": [
        "# **Training Neural Network**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1zUSW9LiGMc"
      },
      "source": [
        "## Installing and Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4PYu6ivkHzjj"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade spotipy torch pandas scikit-learn transformers librosa joblib\n",
        "\n",
        "import spotipy\n",
        "from spotipy.oauth2 import SpotifyClientCredentials\n",
        "import torch\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "import joblib\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrcpPGt7lNnG"
      },
      "source": [
        "## Defining the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-A5yya9BukTg"
      },
      "source": [
        "Here is where the two different models are defined, the first being the default model with all the classifible attributes and the second being the model with the least amount of classifible attributes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "id": "dn3wNUImH1RX"
      },
      "outputs": [],
      "source": [
        "class MusicDataDefault(Dataset):\n",
        "    def __init__(self, path):\n",
        "        data = pd.read_csv(path)\n",
        "        data = data.drop(['track_id'], axis=1)\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        data['track_genre'] = self.label_encoder.fit_transform(data['track_genre'])\n",
        "        self.features = data.drop(['track_genre'], axis=1).values.astype(np.float32)\n",
        "        self.labels = data['track_genre'].values.astype(np.float32)\n",
        "        self.scaler = StandardScaler()\n",
        "        self.features = self.scaler.fit_transform(self.features)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.features[idx]\n",
        "        y = self.labels[idx]\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmcxUDpmdgG3"
      },
      "source": [
        "The way that this model was determined was by taking the default model and removing one attribute at a time to see which attributes where the most important. During these test we had only trained 10 epochs in order to efficiently determine the accuracy of the models. Once the order of most important attribues was determined we added attributes back unitl the accuracy was the same, if not a bit higher, than the origainal model. This allowed us to obtain a model with the highest accuracy with the fewest number of classifible attributes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "id": "6FRmj480y8yw"
      },
      "outputs": [],
      "source": [
        "class MDN(Dataset):\n",
        "    def __init__(self, path):\n",
        "        data = pd.read_csv(path)\n",
        "        data = data.drop(['track_id', 'time_signature', 'liveness', 'key', 'mode',\n",
        "                          'speechiness', 'energy', 'duration_ms', 'instrumentalness'], axis=1)\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        data['track_genre'] = self.label_encoder.fit_transform(data['track_genre'])\n",
        "        self.features = data.drop(['track_genre'], axis=1).values.astype(np.float32)\n",
        "        self.labels = data['track_genre'].values.astype(np.float32)\n",
        "        self.scaler = StandardScaler()\n",
        "        self.features = self.scaler.fit_transform(self.features)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.features[idx]\n",
        "        y = self.labels[idx]\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GiWw8T-lZaS"
      },
      "source": [
        "## Define Underlying MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "id": "VAtGym4giwan"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super(MLP, self).__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(input_dim, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvPvz_LfDMqB"
      },
      "source": [
        "### Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2mvleWfH94k"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "data_path = '/content/drive/My Drive/Junior/Semester_2/NN/Project/dataset.csv'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJ_ql_K7DTFH"
      },
      "source": [
        "### Define Evaluate Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "id": "HA7BG3h0iApZ"
      },
      "outputs": [],
      "source": [
        "# Function to evaluate model performance\n",
        "def evaluate(model, data_loader):\n",
        "    model.eval()\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "    all_predicted = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data_loader:\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_samples += labels.size(0)\n",
        "            total_correct += (predicted == labels.long()).sum().item()\n",
        "            all_predicted.extend(predicted.view(-1).tolist())\n",
        "            all_labels.extend(labels.long().view(-1).tolist())\n",
        "\n",
        "    accuracy = (total_correct / total_samples) * 100\n",
        "    f1 = f1_score(all_labels, all_predicted, average='weighted')\n",
        "    return accuracy, f1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiIrxE9xDWwR"
      },
      "source": [
        "## Neural Network Training Sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZecpXpKjAFX",
        "outputId": "79c29599-3e0d-4662-de91-326002e613f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset MDN, Training Accuracy: 71.50%, Validation Accuracy: 71.75%, Test Accuracy: 72.67%\n"
          ]
        }
      ],
      "source": [
        "# Load datasets\n",
        "datasets = [MDN(data_path)]\n",
        "\n",
        "\n",
        "for dataset in datasets:\n",
        "\n",
        "    # Define best model and model accuracy\n",
        "    best_val_accuracy = 0\n",
        "    best_model_state = None\n",
        "\n",
        "    # Splitting the dataset\n",
        "    total_size = len(dataset)\n",
        "    train_set_size = int(total_size * 0.6)\n",
        "    val_set_size = int(total_size * 0.2)\n",
        "    test_set_size = total_size - (train_set_size + val_set_size)\n",
        "    train_set, val_set, test_set = random_split(dataset, [train_set_size, val_set_size, test_set_size])\n",
        "\n",
        "    train_loader = DataLoader(dataset=train_set, batch_size=64, shuffle=True)\n",
        "    val_loader = DataLoader(dataset=val_set, batch_size=64, shuffle=False)\n",
        "    test_loader = DataLoader(dataset=test_set, batch_size=64, shuffle=False)\n",
        "\n",
        "    # Model, loss, and optimizer\n",
        "    input_dim = dataset.features.shape[1]\n",
        "    num_classes = len(np.unique(dataset.labels))\n",
        "\n",
        "    model = MLP(input_dim, num_classes)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', factor=0.3, patience=5)\n",
        "\n",
        "    # Training loop\n",
        "    num_epochs = 100\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        total_correct = 0\n",
        "        total_samples = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels.long())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_correct += (predicted == labels.long()).sum().item()\n",
        "            total_samples += labels.size(0)\n",
        "\n",
        "        train_accuracy = (total_correct / total_samples) * 100\n",
        "        val_accuracy, val_f1 = evaluate(model, val_loader)\n",
        "\n",
        "        # Adjust the learning rate based on validation accuracy\n",
        "        scheduler.step(val_accuracy)\n",
        "\n",
        "        # Keeps the most accurate model\n",
        "        if val_accuracy > best_val_accuracy:\n",
        "            best_val_accuracy = val_accuracy\n",
        "            best_model_state = model.state_dict()\n",
        "\n",
        "        # Print training and validation results\n",
        "        # print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/total_samples:.4f}, Training Accuracy: {train_accuracy:.2f}%, Validation Accuracy: {val_accuracy:.2f}%, Validation F1: {val_f1:.4f}')\n",
        "\n",
        "    # After training is complete, restore the best model state\n",
        "    model.load_state_dict(best_model_state)\n",
        "\n",
        "    # Evaluate on the test set without loading any saved state\n",
        "    test_accuracy, test_f1 = evaluate(model, test_loader)\n",
        "    print(f'Dataset {dataset.__class__.__name__}, Training Accuracy: {train_accuracy:.2f}%, Validation Accuracy: {val_accuracy:.2f}%, Test Accuracy: {test_accuracy:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNBH324CiPxQ"
      },
      "source": [
        "### Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HMZKelGlHSYO"
      },
      "outputs": [],
      "source": [
        "# Save the model and preprocessors\n",
        "torch.save(model.state_dict(), 'song_genre_classifier.pth')\n",
        "joblib.dump(dataset.label_encoder, 'label_encoder.joblib')\n",
        "joblib.dump(dataset.scaler, 'scaler.joblib')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_XpAhfnDs66"
      },
      "source": [
        "# **Music Classification**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfunDko6DzFz"
      },
      "source": [
        "## Obtain Personal Spotify Playlist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Md4ubr5ID7c6"
      },
      "source": [
        "Get new access token for Spotify account. In order to use this new token you must run the cell, click the link, and then copy the text after 'code='"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4GHuj0RD9wM",
        "outputId": "f205ac96-79fd-4936-ade8-271fc7fbe774"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://accounts.spotify.com/authorize?client_id=cce8687d5317494cb81ff7731ccd9a2b&response_type=code&redirect_uri=https%3A%2F%2Fexample.com%2Fcallback%2F&scope=playlist-modify-private\n"
          ]
        }
      ],
      "source": [
        "client = spotipy.oauth2.SpotifyOAuth(\n",
        "    client_id='cce8687d5317494cb81ff7731ccd9a2b',\n",
        "    client_secret='9646e0745dde4bddb7c67b29431a61f0',\n",
        "    redirect_uri='https://example.com/callback/',\n",
        "    scope='playlist-modify-private'\n",
        ")\n",
        "\n",
        "sp = spotipy.Spotify(auth_manager=client)\n",
        "authorization_url = client.get_authorize_url()\n",
        "print(authorization_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0DX0OQPFs-i"
      },
      "source": [
        "This is where you authorize your account, the code is where you paste the text from the link above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MNXWRiHFjuI"
      },
      "outputs": [],
      "source": [
        "code = \"AQAgXDjXngKs5FyHLs-hoaao4aIQMOJVkA6iqqjQAohdkmNhzuinHaoly0MGby-VVvDAvf8hXtbnwuj8JltaVMQkbk-bppscOq02c_fXRUL3mGpD7ogPNcf7fFp8JqdadpFguhOXtT_rnX__uyG6OcC3l4QgioVXhp6maPs7VWwloRUjHIS2CJqTZR-SCLgl3g-XkTpkHTYiMQ\"\n",
        "client.get_access_token(code)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kqHPdEgF0nc"
      },
      "source": [
        "Getting playlist from my account we want to classify which is the USA Top Songs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {
        "id": "T3SdaU6IGA3e"
      },
      "outputs": [],
      "source": [
        "# Function to get all tracks from a playlist\n",
        "def get_playlist_tracks(playlist_id):\n",
        "    results = sp.playlist_tracks(playlist_id)\n",
        "    tracks = [item['track']['id'] for item in results['items'] if item['track']]\n",
        "    while results['next']:\n",
        "        results = sp.next(results)\n",
        "        tracks.extend([item['track']['id'] for item in results['items'] if item['track']])\n",
        "    return tracks\n",
        "\n",
        "# Get all tracks from a specific playlist\n",
        "playlist_id = '3qILVAqgqMNH18dPJPvan1'\n",
        "song_ids = get_playlist_tracks(playlist_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIv3cCkiGyo6"
      },
      "source": [
        "Here we are creating a dictionary of the songs within that playlist. The dictionary contains the key, which is the song_id, and then the song name, artist, and genre."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fzOQ6eyCG3m0"
      },
      "outputs": [],
      "source": [
        "# Load the model and preprocessors\n",
        "model = MLP(input_dim, num_classes)\n",
        "model.load_state_dict(torch.load('song_genre_classifier.pth'))\n",
        "model.eval()\n",
        "\n",
        "scaler = joblib.load('scaler.joblib')\n",
        "label_encoder = joblib.load('label_encoder.joblib')\n",
        "\n",
        "results = {}\n",
        "\n",
        "for song_id in song_ids:\n",
        "    song_features = sp.audio_features(song_id)[0]\n",
        "    track_info = sp.track(song_id)\n",
        "    song_name = track_info['name']\n",
        "    artist_name = track_info['artists'][0]['name']\n",
        "    popularity = track_info['popularity']\n",
        "\n",
        "    # Ensure these features match the training set's features\n",
        "    features_list = ['danceability', 'loudness', 'acousticness', 'valence', 'tempo']\n",
        "    features_values = [song_features[feature] for feature in features_list]\n",
        "    features_values.append(popularity)\n",
        "    features_array = np.array([features_values], dtype=np.float32)\n",
        "    scaled_features = scaler.transform(features_array)\n",
        "\n",
        "    input_tensor = torch.tensor(scaled_features, dtype=torch.float32)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_tensor)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        genre = label_encoder.inverse_transform(predicted.numpy())[0]\n",
        "\n",
        "    results[song_id] = {\n",
        "        'song_name': song_name,\n",
        "        'artist_name': artist_name,\n",
        "        'genre': genre\n",
        "    }\n",
        "\n",
        "for song_id, info in results.items():\n",
        "    print(f\"{info['song_name']} by {info['artist_name']}, Genre: {info['genre']}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "19C115YOikRXYQDQWR5GP1cXfjArWMqmN",
      "authorship_tag": "ABX9TyP+PyOHbIrSWHuuStNrjOGy",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}